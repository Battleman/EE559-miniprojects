
@inproceedings{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html},
	urldate = {2020-05-16},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	pages = {1026--1034},
	file = {Full Text PDF:/home/battleman/Documents/Zotero/storage/VAZKSQF6/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf;Snapshot:/home/battleman/Documents/Zotero/storage/YR23ZKFE/He_Delving_Deep_into_ICCV_2015_paper.html:text/html}
}

@article{kumar_weight_2017,
	title = {On weight initialization in deep neural networks},
	url = {http://arxiv.org/abs/1704.08863},
	abstract = {A proper initialization of the weights in a neural network is critical to its convergence. Current insights into weight initialization come primarily from linear activation functions. In this paper, I develop a theory for weight initializations with non-linear activations. First, I derive a general weight initialization strategy for any neural network using activation functions differentiable at 0. Next, I derive the weight initialization strategy for the Rectified Linear Unit (RELU), and provide theoretical insights into why the Xavier initialization is a poor choice with RELU activations. My analysis provides a clear demonstration of the role of non-linearities in determining the proper weight initializations.},
	urldate = {2020-05-16},
	journal = {arXiv:1704.08863 [cs]},
	author = {Kumar, Siddharth Krishna},
	month = may,
	year = {2017},
	note = {arXiv: 1704.08863},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 9 pages, 4 figures},
	file = {arXiv Fulltext PDF:/home/battleman/Documents/Zotero/storage/7Q6UTATI/Kumar - 2017 - On weight initialization in deep neural networks.pdf:application/pdf;arXiv.org Snapshot:/home/battleman/Documents/Zotero/storage/JIWRD9JU/1704.html:text/html}
}

@misc{noauthor_pytorch_nodate,
	title = {{PyTorch}},
	url = {https://www.pytorch.org},
	abstract = {An open source deep learning platform that provides a seamless path from research prototyping to production deployment.},
	language = {en},
	urldate = {2020-05-19},
	note = {Library Catalog: pytorch.org},
	file = {Snapshot:/home/battleman/Documents/Zotero/storage/LT28L7UC/pytorch.org.html:text/html}
}

@misc{noauthor_tensorflow_nodate,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/},
	abstract = {An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources.},
	language = {en},
	urldate = {2020-05-19},
	journal = {TensorFlow},
	note = {Library Catalog: www.tensorflow.org},
	file = {Snapshot:/home/battleman/Documents/Zotero/storage/WPING5UJ/www.tensorflow.org.html:text/html}
}

@article{postalcioglu_performance_2019,
	title = {Performance {Analysis} of {Different} {Optimizers} for {Deep} {Learning}-{Based} {Image} {Recognition}},
	volume = {34},
	issn = {0218-0014},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218001420510039},
	doi = {10.1142/S0218001420510039},
	abstract = {Deep learning refers to Convolutional Neural Network (CNN). CNN is used for image recognition for this study. The dataset is named Fruits-360 and it is obtained from the Kaggle dataset. Seventy percent of the pictures are selected as training data and the rest of the images are used for testing. In this study, an image size is 100×100×3100×100×3{\textless}math display="inline" overflow="scroll" altimg="eq-00001.gif"{\textgreater}{\textless}mn{\textgreater}1{\textless}/mn{\textgreater}{\textless}mn{\textgreater}0{\textless}/mn{\textgreater}{\textless}mn{\textgreater}0{\textless}/mn{\textgreater}{\textless}mo{\textgreater}×{\textless}/mo{\textgreater}{\textless}mn{\textgreater}1{\textless}/mn{\textgreater}{\textless}mn{\textgreater}0{\textless}/mn{\textgreater}{\textless}mn{\textgreater}0{\textless}/mn{\textgreater}{\textless}mo{\textgreater}×{\textless}/mo{\textgreater}{\textless}mn{\textgreater}3{\textless}/mn{\textgreater}{\textless}/math{\textgreater}. Training is realized using Stochastic Gradient Descent with Momentum (sgdm), Adaptive Moment Estimation (adam) and Root Mean Square Propogation (rmsprop) techniques. The threshold value is determined as 98\% for the training. When the accuracy reaches more than 98\%, training is stopped. Calculation of the final validation accuracy is done using trained network. In this study, more than 98\% of the predicted labels match the true labels of the validation set. Accuracies are calculated using test data for sgdm, adam and rmsprop techniques. The results are 98.08\%, 98.85\%, 98.88\%, respectively. It is clear that fruits are recognized with good accuracy.},
	number = {02},
	urldate = {2020-05-19},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	author = {Postalcıoğlu, Seda},
	month = apr,
	year = {2019},
	note = {Publisher: World Scientific Publishing Co.},
	pages = {2051003},
	file = {Snapshot:/home/battleman/Documents/Zotero/storage/L8AA9FZL/S0218001420510039.html:text/html}
}
